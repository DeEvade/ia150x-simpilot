version: "3.8"

services:
  qwen-server:
    image: vllm/vllm-openai:latest
    container_name: qwen-server
    platform: linux/amd64/v8
    ports:
      - "8000:8080"
    environment:
      - HUGGING_FACE_HUB_TOKEN=test # vLLM token (if required)
    command: > # Runs the vLLM command
      --model Qwen/Qwen2.5-3B-Instruct
      --dtype float16
      --device cpu
      --disable-async-output-proc

    ipc: host
